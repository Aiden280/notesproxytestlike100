---
title: chatgpt
created: '2022-12-06T07:37:08.000Z'
modified: '2022-12-08T16:49:29.938Z'
---

# chatgpt

this gpt seems really good. currently only api access.

## projects related to chatgpt

[chatgpt use cases curated list](https://github.com/jqueryscript/ChatGPT)

[DAILA](https://github.com/mahaloz/DAILA) use chatgpt to identify function calls in decompiler

## access via api

https://github.com/altryne/chatGPT-telegram-bot

https://github.com/taranjeet/chatgpt-api

https://github.com/acheong08/ChatGPT

https://github.com/vincelwt/chatgpt-mac

https://github.com/transitive-bullshit/chatgpt-api

https://github.com/rawandahmad698/PyChatGPT

## models like chatgpt

[gpt-gmlp](https://github.com/lucidrains/g-mlp-gpt) saying this design integrates gpt with gmlps so will use less ram and can be trained on a single gpu

[WebGPT](https://github.com/James4Ever0/webgpt-cli)

[tk-instruct](https://github.com/yizhongw/Tk-Instruct) with [all models](https://huggingface.co/models?search=allenai/tk-instruct) by allenai can be [multilingual](https://huggingface.co/allenai/mtk-instruct-11b-def-pos), trained on [natural instructions](https://instructions.apps.allenai.org/)

there's a ghosted repo named [instructgpt-pytorch](https://github.com/mariusmcl/instructgpt-pytorch) found in bing but no cache preserved, also an empty repo called [InstructFNet](https://github.com/flippe3/InstructFNet) wtf?

[AidMe](https://github.com/nicolas-lair/AidMe) Code and experiment of the article AidMe User-in-the-loop Adaptative Intent Detecttion for Instructable Digital Assistant

[cheese](https://github.com/CarperAI/cheese) Used for adaptive human in the loop evaluation of language and embedding models.

[Kelpie](https://github.com/AndRossi/Kelpie) Explainable AI framework for interpreting Link Predictions on Knowledge Graphs

[GrIPS](https://github.com/archiki/GrIPS)  Gradient-free, Edit-based Instruction Search for Prompting Large Language Models

[queakily](https://github.com/CarperAI/squeakily) nlp datasets cleaner

gpt-j

[multi-modal deeplearning](https://github.com/JingfengYang/Multi-modal-Deep-Learning) paper collections

[bloom](https://huggingface.co/docs/transformers/model_doc/bloom) a huge model like gpt-3

notice, gpt-2 is somehow inferior to gpt-3 since it has smaller model parameters

