---
title: chatgpt
created: '2022-12-06T07:37:08.000Z'
modified: '2022-12-17T21:27:51.076Z'
---

# chatgpt

this gpt seems really good. currently only api access.

## analysis about how to make such model

two similar models by openai: webgpt and instructgpt

## projects related to chatgpt

[chatgpt use cases curated list](https://github.com/jqueryscript/ChatGPT)

[DAILA](https://github.com/mahaloz/DAILA) use chatgpt 
to identify function calls in decompiler

[awesome transformer language models](https://github.com/cedrickchee/awesome-transformer-nlp/blob/f2dbb46a382a30dd3d23d88c1b8d826ba5c34aab/README.md) a huge collection on transformer based LMs, huge models by megacorps, with some introduction and analogy on chatgpt

[huggingface blog](https://github.com/huggingface/blog) on [RLHF](https://github.com/huggingface/blog/blob/dfbaec824b313f88581c761bc10c8943226970be/rlhf.md) containing similar projects and source code

bilibili sends me lots of videos (and articles) on hacking and ai (including chatgpt) via its android app. recommend you to scrape this source and collect transcription and screenshots for searching and content generation.

b站有做免杀 绕过杀软的

[chatgpt原理解析](https://bilibili.com/video/BV1B14y1K7AP)

chatgpt对接搜索引擎

下载链接:
github: https://github.com/josStorer/chat-gpt-search-engine-extension/releases/
百度网盘: https://pan.baidu.com/s/1MnFJTDIatyIIPr5kUMWsAw?pwd=1111 
提取码：1111

原项目: https://github.com/wong2/chat-gpt-google-extension
我创建的fork, 添加了多个搜索引擎支持的版本: https://github.com/josStorer/chat-gpt-search-engine-extension
PR: https://github.com/wong2/chat-gpt-google-extension/pull/31

已修复先前百度需要手动刷新的问题

## access via api

https://github.com/altryne/chatGPT-telegram-bot

https://github.com/taranjeet/chatgpt-api

https://github.com/acheong08/ChatGPT

https://github.com/vincelwt/chatgpt-mac

https://github.com/transitive-bullshit/chatgpt-api

https://github.com/rawandahmad698/PyChatGPT

## models like chatgpt

[lm-human-preferences](https://github.com/openai/lm-human-preferences) by openai

[trl](https://github.com/lvwerra/trl) Train transformer language models with reinforcement learning based on gpt2

[trix](https://github.com/CarperAI/trlx) A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF) by CarperAI

[RL4LMs](https://github.com/allenai/RL4LMs) A modular RL library to fine-tune language models to human preferences

[PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch) saying this is basically chatgpt with palm

[gpt-gmlp](https://github.com/lucidrains/g-mlp-gpt) saying this design integrates gpt with gmlps so will use less ram and can be trained on a single gpu

[WebGPT](https://github.com/James4Ever0/webgpt-cli)

[tk-instruct](https://github.com/yizhongw/Tk-Instruct) with [all models](https://huggingface.co/models?search=allenai/tk-instruct) by allenai can be [multilingual](https://huggingface.co/allenai/mtk-instruct-11b-def-pos), trained on [natural instructions](https://instructions.apps.allenai.org/)

there's a ghosted repo named [instructgpt-pytorch](https://github.com/mariusmcl/instructgpt-pytorch) found in bing but no cache preserved, also an empty repo called [InstructFNet](https://github.com/flippe3/InstructFNet) wtf?

[AidMe](https://github.com/nicolas-lair/AidMe) Code and experiment of the article AidMe User-in-the-loop Adaptative Intent Detecttion for Instructable Digital Assistant

[cheese](https://github.com/CarperAI/cheese) Used for adaptive human in the loop evaluation of language and embedding models.

[Kelpie](https://github.com/AndRossi/Kelpie) Explainable AI framework for interpreting Link Predictions on Knowledge Graphs

[GrIPS](https://github.com/archiki/GrIPS)  Gradient-free, Edit-based Instruction Search for Prompting Large Language Models

[queakily](https://github.com/CarperAI/squeakily) nlp datasets cleaner

gpt-j

super big bilingual model [GLM-130B](https://github.com/THUDM/GLM-130B)

[multi-modal deeplearning](https://github.com/JingfengYang/Multi-modal-Deep-Learning) paper collections

[bloom](https://huggingface.co/docs/transformers/model_doc/bloom) a huge model like gpt-3

notice, gpt-2 is somehow inferior to gpt-3 since it has smaller model parameters

[dialogue-generation](https://github.com/bme-chatbots/dialogue-generation) Generating responses with pretrained XLNet and GPT-2 in PyTorch.

[personaGPT](https://github.com/illidanlab/personaGPT) Implementation of PersonaGPT Dialog Model

[DialoGPT](https://github.com/microsoft/DialoGPT) Large-scale pretraining for dialogue
